import requests
import time
import csv
import re
import os
import signal
import sys
import random
from datetime import datetime

# ==============================
#  å…¨å±€å˜é‡
# ==============================
stop_crawling = False  # ç”¨äº Ctrl+C ä¸­æ–­æ§åˆ¶
CHECKPOINT_INTERVAL = 1000  # æ¯å¤šå°‘æ¡ä¿å­˜ä¸€æ¬¡æ–­ç‚¹


# ==============================
#  ä¿¡å·å¤„ç†
# ==============================
def signal_handler(sig, frame):
    """å¤„ç† Ctrl+C ä¿¡å·"""
    global stop_crawling
    print("\næ£€æµ‹åˆ° Ctrl+Cï¼Œæ­£åœ¨å®‰å…¨åœæ­¢çˆ¬å–...")
    stop_crawling = True


# ==============================
#  å·¥å…·å‡½æ•°
# ==============================
def clean_text(text):
    """æ¸…ç†è¯„è®ºå†…å®¹ï¼Œå»é™¤æ¢è¡Œç¬¦ã€emoji ç­‰"""
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"[^\u0000-\uFFFF]", "", text)
    return text.strip()


def get_bvid_from_url(url):
    """ä»Bç«™è§†é¢‘URLä¸­æå– bvid"""
    pattern = r"BV[0-9A-Za-z]{10}"
    match = re.search(pattern, url)
    return match.group(0) if match else None


def get_with_retry(url, params=None, headers=None, cookies=None, retries=3):
    """å¸¦é‡è¯•æœºåˆ¶çš„è¯·æ±‚"""
    for attempt in range(retries):
        try:
            response = requests.get(
                url, params=params, headers=headers, cookies=cookies, timeout=10
            )
            response.raise_for_status()
            return response
        except requests.RequestException as e:
            print(f"è¯·æ±‚å¤±è´¥ï¼ˆç¬¬ {attempt+1} æ¬¡ï¼‰ï¼š{e}")
            time.sleep(random.uniform(1.5, 3.5))
    return None


# ==============================
#  Bç«™APIå‡½æ•°
# ==============================
def get_video_info(bvid):
    """è·å–è§†é¢‘ä¿¡æ¯"""
    url = f"https://api.bilibili.com/x/web-interface/view?bvid={bvid}"
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": f"https://www.bilibili.com/video/{bvid}",
    }
    response = get_with_retry(url, headers=headers)
    if not response:
        print("è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥")
        return None
    data = response.json()
    if data["code"] == 0:
        return data["data"]
    else:
        print(f"è§†é¢‘ä¿¡æ¯è¯·æ±‚å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
        return None


def get_comments(aid, page=1, session_cookie=None):
    def get_comments(aid, page=1, session_cookie=None):
        "è·å–æŒ‡å®šé¡µæ•°çš„è¯„è®º"
    url = "https://api.bilibili.com/x/v2/reply"
    params = {
        "pn": page,
        "type": 1,
        "oid": aid,
        "sort": 1,  # æ”¹ä¸ºæŒ‰æ—¶é—´å€’åºï¼Œé£æ§æ›´ä½
    }

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/120.0.0.0 Safari/537.36",
        "Referer": f"https://www.bilibili.com/video/av{aid}",
        "Origin": "https://www.bilibili.com",
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "Connection": "keep-alive",
    }

    cookies = {"SESSDATA": session_cookie} if session_cookie else None
    response = get_with_retry(url, params=params, headers=headers, cookies=cookies)
    return response.json() if response else None

# ==============================
#  æ•°æ®è§£æå‡½æ•°
# ==============================
def parse_comment(comment):
    """æå–å•æ¡è¯„è®ºä¿¡æ¯"""
    return {
        "comment_id": comment["rpid"],
        "user_id": comment["member"]["mid"],
        "user_name": comment["member"]["uname"],
        "content": clean_text(comment["content"]["message"]),
        "like_count": comment["like"],
        "reply_count": comment["count"],
        "timestamp": comment["ctime"],
        "time": datetime.fromtimestamp(comment["ctime"]).strftime("%Y-%m-%d %H:%M:%S"),
    }


def extract_replies(comment):
    """é€’å½’æå–æ¥¼ä¸­æ¥¼è¯„è®º"""
    replies = []
    if "replies" in comment and comment["replies"]:
        for reply in comment["replies"]:
            replies.append(parse_comment(reply))
            replies.extend(extract_replies(reply))
    return replies


# ==============================
#  æ ¸å¿ƒçˆ¬å–é€»è¾‘
# ==============================
def crawl_all_comments(video_url, session_cookie=None, save_path=None, max_pages=None, max_comments=None):
    """çˆ¬å–æŒ‡å®šè§†é¢‘çš„å…¨éƒ¨è¯„è®º"""
    global stop_crawling
    stop_crawling = False
    signal.signal(signal.SIGINT, signal_handler)

    bvid = get_bvid_from_url(video_url)
    if not bvid:
        print("âŒ æ— æ³•ä»URLæå–BVå·")
        return

    video_info = get_video_info(bvid)
    if not video_info:
        print("âŒ è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥")
        return

    aid = video_info["aid"]
    title = video_info["title"]
    print(f"\nğŸ¬ å¼€å§‹çˆ¬å–: {title} (aid={aid}, bvid={bvid})")
    print("æŒ‰ Ctrl+C å¯éšæ—¶åœæ­¢")

    # æ–­ç‚¹æ–‡ä»¶è·¯å¾„
    checkpoint_file = f"bilibili_comments_{bvid}_checkpoint.txt"
    start_page = 1

    if os.path.exists(checkpoint_file):
        try:
            with open(checkpoint_file, "r") as f:
                start_page = int(f.read().strip())
            print(f"ğŸ“ æ£€æµ‹åˆ°æ–­ç‚¹æ–‡ä»¶ï¼Œä»ç¬¬ {start_page} é¡µç»§ç»­çˆ¬å–")
        except:
            pass

    # è¾“å‡ºæ–‡ä»¶
    if save_path:
        os.makedirs(save_path, exist_ok=True)
        filename = os.path.join(save_path, f"bilibili_comments_{bvid}.csv")
    else:
        filename = f"bilibili_comments_{bvid}.csv"

    # åˆå§‹åŒ– CSV
    is_new_file = not os.path.exists(filename)
    f = open(filename, "a", newline="", encoding="utf-8-sig")
    writer = csv.DictWriter(
        f,
        fieldnames=[
            "comment_id",
            "user_id",
            "user_name",
            "content",
            "like_count",
            "reply_count",
            "timestamp",
            "time",
        ],
    )
    if is_new_file:
        writer.writeheader()

    # ç¬¬ä¸€é¡µè¯·æ±‚
    first_page = get_comments(aid, start_page, session_cookie)
    if not first_page or first_page.get("code") != 0:
        print("âŒ è·å–ç¬¬ä¸€é¡µè¯„è®ºå¤±è´¥")
        return

    total_comments = first_page["data"]["page"]["count"]
    total_pages = (total_comments + 19) // 20
    print(f"ğŸ“Š æ€»è¯„è®ºæ•°ï¼š{total_comments}ï¼Œé¢„è®¡é¡µæ•°ï¼š{total_pages}")

    if max_pages and max_pages < total_pages:
        total_pages = max_pages
        print(f"âš™ï¸ åº”ç”¨æœ€å¤§é¡µæ•°é™åˆ¶ï¼š{max_pages}")

    # çˆ¬å–å¾ªç¯
    page = start_page
    total_saved = 0
    while page <= total_pages and not stop_crawling:
        print(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page}/{total_pages} é¡µ...")
        data = get_comments(aid, page, session_cookie)
        if not data or data["code"] != 0:
            print(f"âš ï¸ ç¬¬ {page} é¡µçˆ¬å–å¤±è´¥ï¼Œè·³è¿‡")
            page += 1
            continue

        replies = data["data"].get("replies", [])
        if not replies:
            print("ğŸŒ€ æ— æ›´å¤šè¯„è®º")
            break

        page_comments = []
        for comment in replies:
            page_comments.append(parse_comment(comment))
            page_comments.extend(extract_replies(comment))

        writer.writerows(page_comments)
        total_saved += len(page_comments)

        # ä¿å­˜æ–­ç‚¹
        if total_saved % CHECKPOINT_INTERVAL < len(page_comments):
            with open(checkpoint_file, "w") as f_ck:
                f_ck.write(str(page))

        print(f"âœ… å·²ä¿å­˜ {total_saved} æ¡è¯„è®º")
        if max_comments and total_saved >= max_comments:
            print(f"ğŸ¯ è¾¾åˆ°æœ€å¤§è¯„è®ºæ•° {max_comments} æ¡ï¼Œåœæ­¢")
            break

        page += 1
        time.sleep(random.uniform(0.5, 1.5))

    # æ”¶å°¾
    f.close()
    if os.path.exists(checkpoint_file):
        os.remove(checkpoint_file)

    print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼Œå…±ä¿å­˜ {total_saved} æ¡è¯„è®º")
    print(f"ğŸ“ æ–‡ä»¶ä¿å­˜åˆ°: {os.path.abspath(filename)}")


# ==============================
#  ä¸»ç¨‹åºå…¥å£
# ==============================
if __name__ == "__main__":
    video_url = "https://www.bilibili.com/video/BV1JQsoz3EtK"  # ç¤ºä¾‹
    save_path = "results"
    session_cookie = "41c8a2ef%2C1777189115%2C9efbd%2Aa1CjBNQCv6ZvU3UDdyf01GCeTfyytrG_9H3EVu4h6HjxHNOwrGBTZW4ugEqN73VG1gBggSVjY4WFRGb293T1Y4OURpUnJoVkpfQ2F4WUhjcFk3SWxIU080YlB4clhua0tfa1BjWEwtVFRjRnpvVXFHNWdpMGZjS0t0V3d6VUtrWGJ4X1hGeWJOWHRnIIEC"  # å¯å¡«å…¥ä½ çš„ SESSDATA

    crawl_all_comments(
        video_url,
        session_cookie=session_cookie,
        save_path=save_path,
        max_pages=None,
        max_comments=5000,
    )